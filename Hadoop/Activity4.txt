root@08860a8b6d78:~# cd /
root@08860a8b6d78:/# hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 3c47eab0-7114-43de-990e-ebe2e94ed68a

Logging initialized using configuration in jar:file:/usr/local/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = e26c2c69-a4fa-40c0-8185-135fd87e85a5
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> clear
    >
    > hive
    > ;
NoViableAltException(24@[])
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:0 cannot recognize input near 'clear' 'hive' '<EOF>'
hive> show databases;
OK
default
Time taken: 0.729 seconds, Fetched: 1 row(s)
hive> show tables;
OK
Time taken: 0.186 seconds
hive> descrive OK
    > ;
NoViableAltException(24@[])
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:0 cannot recognize input near 'descrive' 'OK' '<EOF>'
hive> describe OK;
FAILED: SemanticException [Error 10001]: Table not found OK
hive> CREATE TABLE files;
FAILED: SemanticException [Error 10043]: Either list of columns or a custom serializer should be specified
hive> CREATE TABLE files
    > (lines STRING);
OK
Time taken: 0.908 seconds
hive> show tables;
OK
files
Time taken: 0.055 seconds, Fetched: 1 row(s)
hive> DESCRIBE files;
OK
lines                   string
Time taken: 0.21 seconds, Fetched: 1 row(s)
hive> LOAD DATA LOCAL INPATH 'root/txtFile.txt' INTO TABLE files;
Loading data to table default.files
OK
Time taken: 1.633 seconds
hive> SELECT * FROM files;
OK
Night. That over years creepeth green fourth had after also seas make. Female living sea very hath. Dry good in is bring fifth under creeping all tree land.
Time taken: 3.0 seconds, Fetched: 1 row(s)
hive> dfs -ls /user/hive
    > ;
Found 1 items
drwxr-xr-x   - root supergroup          0 2023-05-11 06:11 /user/hive/warehouse
hive> CREATE TABLE word_count AS
    > SELECT word, COUNT(1) AS wordCount FROM
    > (SELECT explode(split(lines, ' ')) AS word FROM files) words
    > GROUP BY word
    > ORDER BY wordCount;
Query ID = root_20230511064531_b15a479b-c6f1-4c0a-a03d-02063b007451
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1683618626966_0002, Tracking URL = http://08860a8b6d78:8088/proxy/application_1683618626966_0002/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1683618626966_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-05-11 06:46:01,581 Stage-1 map = 0%,  reduce = 0%
2023-05-11 06:46:18,521 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 12.94 sec
2023-05-11 06:46:27,032 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.61 sec
MapReduce Total cumulative CPU time: 17 seconds 610 msec
Ended Job = job_1683618626966_0002
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1683618626966_0003, Tracking URL = http://08860a8b6d78:8088/proxy/application_1683618626966_0003/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1683618626966_0003
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-05-11 06:46:46,037 Stage-2 map = 0%,  reduce = 0%
2023-05-11 06:46:54,384 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.74 sec
2023-05-11 06:47:02,711 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.56 sec
MapReduce Total cumulative CPU time: 8 seconds 560 msec
Ended Job = job_1683618626966_0003
Moving data to directory hdfs://08860a8b6d78:9000/user/hive/warehouse/word_count
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 17.61 sec   HDFS Read: 8313 HDFS Write: 757 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 8.56 sec   HDFS Read: 7892 HDFS Write: 288 SUCCESS
Total MapReduce CPU Time Spent: 26 seconds 170 msec
OK
Time taken: 93.957 seconds
hive> SELECT * FROM word_count
    > ;
OK
years   1
very    1
under   1
tree    1
seas    1
sea     1
over    1
make.   1
living  1
land.   1
is      1
in      1
hath.   1
had     1
green   1
good    1
fourth  1
fifth   1
creeping        1
creepeth        1
bring   1
also    1
all     1
after   1
That    1
Night.  1
Female  1
Dry     1
Time taken: 0.177 seconds, Fetched: 28 row(s)
hive> EXPLAIN CREATE TABLE word_count AS
    > SELECT word, COUNT(1) AS wordCount FROM
    > (SELECT explode(split(lines, ' ')) AS word FROM files) words
    > GROUP BY word
    > ORDER BY wordCount;
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-4 depends on stages: Stage-0
  Stage-3 depends on stages: Stage-4

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: files
            Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: split(lines, ' ') (type: array<string>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
              UDTF Operator
                Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
                function name: explode
                Group By Operator
                  aggregations: count()
                  keys: col (type: string)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col0 (type: string)
                    Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col1 (type: bigint)
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col1 (type: bigint)
              sort order: +
              Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: string)
      Execution mode: vectorized
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: bigint)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 1570 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.word_count

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
          destination: hdfs://08860a8b6d78:9000/user/hive/warehouse/word_count

  Stage: Stage-4
      Create Table Operator:
        Create Table
          columns: word string, wordcount bigint
          input format: org.apache.hadoop.mapred.TextInputFormat
          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.word_count

  Stage: Stage-3
    Stats Work
      Basic Stats Work:

Time taken: 0.41 seconds, Fetched: 90 row(s)
hive>